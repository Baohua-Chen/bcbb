#+BLOG: bcbio
#+POSTID: 702
#+TITLE: Benchmarking variation and RNA-seq analyses on Amazon Web Services with Docker
#+CATEGORY: benchmarking
#+TAGS: bioinformatics, variant, ngs, validation, benchmarking, collectl
#+OPTIONS: toc:nil num:nil

* Overview

We developed a freely available, easy to run implementation of [[bcbio][bcbio-nextgen]] on
Amazon Web Services (AWS) using [[docker][Docker]]. bcbio is a community developed tool
providing validated and scalable variant calling and RNA-seq analysis. The AWS
implementation automates all of the steps of building a cluster, attaching high
performance shared filesystems, and running an analysis. This makes bcbio
readily available to the research community without the need to install and
configure a local installation.

We provide timing benchmarks for running a full variant calling analysis using
[[bcbio][bcbio]] on AWS. The benchmark dataset was [[dream_about][a cancer tumor/normal evaluation]], from
[[dream][the ICGC-TCGA DREAM challenge]], with 100x coverage in exome regions. We compared
the results of running this dataset on 3 different networked filesystems:
Lustre, NFS and Gluster. We also have a ready to run example RNA-seq dataset
using inputs from [[seqc_paper][the Sequencing Quality Control (SEQC) project]].

The entire installation bootstraps from standard Ubuntu AMIs, enabling
adjustment of the tools, genome data and installation without needing to
re-prepare AMIs. The implementation uses [[elasticluster][Elasticluster]] to provision and
configure the cluster. We automate the process with
[[boto][the boto Python interface to AWS]] and [[ansible][Ansible scripts]]. [[bcbiovm][bcbio-vm]] isolates
code and tools inside a [[docker][Docker]] container allowing runs on any remote machine
with a download of the Docker image and access to the shared filesystem.
Analyses run directly from S3 buckets, with automatic streaming download
of input data and upload of final processed data.

#+LINK: bcbio http://github.com/chapmanb/bcbio-nextgen
#+LINK: aws http://aws.amazon.com/
#+LINK: docker https://docker.com/
#+LINK: dream https://www.synapse.org/#!Synapse:syn312572
#+LINK: dream_about https://bcbio-nextgen.readthedocs.org/en/latest/contents/testing.html#cancer-tumor-normal
#+LINK: boto http://boto.readthedocs.org/en/latest/
#+LINK: bcbiovm https://github.com/chapmanb/bcbio-nextgen-vm

Using this setup to benchmark analysis runs on AWS, we found that:

- The AWS setup outperformed local runs, especially in IO intensive steps
  like alignment post-processing. AWS offers the opportunity to rent SSD backed
  storage and setup a cluster without contention for network resources. Our
  local test is on an in-production Lustre filesystem attached to a large highly
  utilized cluster provided by [[fas][Harvard FAS research computing]].

- Lustre outperforms Gluster/NFS due to improved speed during
  alignments. Measuring system resource usage with collectl, there are two primary
  difference between Lustre and Gluster/NFS runs during this step. The
  Gluster/NFS filesystems have more system CPU usage, corresponding to work by
  the glusterfs daemon on the compute nodes. Additionally, there is more network
  traffic for both Gluster and NFS during these step.

- Gluster underperforms Lustre and NFS due to a slowdown during the
  calculation of callable regions. Practically this step involves
  reads from bgzipped compressed BAM files. We don't have a good measure of why
  this slows down from the metric plots, but it seems likely to be due to contention
  reading the BAM files since the callable metrics get calculated in parallel
  for each chromosome.

We developed bcbio on AWS and ran these timing benchmarks with a lot of
brilliant help:

- John Morrissey automated the process of starting a bcbio cluster on AWS and
  attaching a Lustre filesystem. He also automated the approach to generating
  graphs of resource usage from collectl stats and provided critical front line
  and testing and improvements to all the components of the bcbio AWS
  interaction.

- Kristina Kermanshahche and Robert Read with Intel provided great support
  helping us get the [[icel][Lustre ICEL CloudFormation]] templates running.

- Ronen Artzi, Michael Heimlich, and Justin Johnson at [[az][AstraZenenca]] setup the
  Lustre, Gluster and NFS benchmarks using a bcbio [[starcluster][StarCluster]] instance. This
  initial validation was essential for convincing us of the value of moving to a
  shared filesystem on AWS.

- Jason Tetrault, Karl Gutwin and Hank Wu at [[biogen][Biogen]] provided valuable feedback,
  suggestions and resources for developing bcbio on AWS.

- Glen Otero parsed the collectl data and provided graphs, which gave us a
  detailed look into the potential causes of bottlenecks we found in the
  timings.

- James Cuff, Paul Edmon and the team at [[fas][Harvard FAS research computing]]
  built and administered the Regal Lustre setup used for local testing.

- John Kern and other members of the bcbio community tested, debugged and helped
  identify issues with the implementation. Feedback and contributions are the
  backbone of bcbio development.

#+LINK: icel https://wiki.hpdd.intel.com/display/PUB/Intel+Cloud+Edition+for+Lustre*+Software
#+LINK: fas https://rc.fas.harvard.edu/
#+LINK: az http://www.astrazeneca.com
#+LINK: biogen http://www.biogenidec.com/

* Reproducing the analysis

The process to launch the cluster and an NFS or optional Lustre shared
filesystem is [[awsdocs][fully automated and documented]]. It sets up permissions, VPCs,
clusters and shared filesystems from a basic AWS account, so requires minimal
manual work. ~bcbio_vm.py~ has commands to:

- Add an IAM user, a VPC and create the Elasticluster config.
- Launch a cluster and bootstrap with the latest bcbio code and data.
- Create and mount a Lustre filesystem attached to the cluster
- Terminate the cluster and Lustre stack upon completion.

The processing handles download of input data from S3 and upload back to S3 on
finalization. We store data encrypted on S3 and manage access using
[[instance_profile][IAM instance profiles]]. The examples below show how to run both a somatic variant
calling evaluation and an RNA-seq evaluation.

#+LINK: starcluster http://star.mit.edu/cluster/index.html
#+LINK: elasticluster https://github.com/gc3-uzh-ch/elasticluster
#+LINK: ansible http://www.ansible.com/home
#+LINK: awsdocs https://bcbio-nextgen.readthedocs.org/en/latest/contents/cloud.html
#+LINK: instance_profile http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html

** Running a somatic variant calling evaluation

This analysis performs evaluation of variant calling using
[[dream_about][tumor/normal somatic sample from the DREAM challenge]].
To run, prepare an S3 bucket to run the analysis from. Copy the [[evalconfig][configuration file]]
to your own personal bucket and add a [[gatk][GATK]] jar. You can use the AWS console or
any available S3 client to do this. For example, using the [[awscli][AWS command line client]]:

#+BEGIN_SRC
aws s3 mb s3://YOURBUCKET-syn3-eval/
aws s3 cp s3://bcbio-syn3-eval/cancer-dream-syn3-aws.yaml s3://YOURBUCKET-syn3-eval/
aws s3 cp GenomeAnalysisTK.jar s3://YOURBUCKET-syn3-eval/jars/
#+END_SRC
#+BEGIN_HTML
<br/>
#+END_HTML

Now ssh to the cluster head node, create the work directory and use bcbio_vm to
create a batch script that we submit to SLURM. This example uses an attacked
Lustre filesystem:

#+BEGIN_SRC
bcbio_vm.py elasticluster ssh bcbio
sudo mkdir -p/scratch/cancer-dream-syn3-exome
sudo chown ubuntu !$
cd !$ && mkdir work && cd work
bcbio_vm.py ipythonprep s3://YOURBUCKET-syn3-eval/cancer-dream-syn3-aws.yaml \
                        slurm cloud -r 'mincores=30' -r 'timelimit=2-00:00:00' -n 60
sbatch bcbio_submit.sh
#+END_SRC
#+BEGIN_HTML
<br/>
#+END_HTML

This runs alignment and variant calling with multiple callers (MuTect,
FreeBayes, VarDict and VarScan), validates against the
[[dream][DREAM validation dataset truth calls]] and uploads the results back
to S3 in YOURBUCKET-syn3-eval/final.

#+LINK: evalconfig https://s3.amazonaws.com/bcbio-syn3-eval/cancer-dream-syn3-aws.yaml
#+LINK: awscli https://aws.amazon.com/cli/
#+LINK: gatk https://www.broadinstitute.org/gatk/

** Running a RNA-seq evaluation

This example runs an RNA-seq analysis using inputs from
[[seqc_paper][the Sequencing Quality Control (SEQC) project]]. Full details on the analysis are
available in the [[seqc_details][bcbio example run documentation]]. To setup the run, we copy the
input configuration from a publicly available S3 bucket into your own personal bucket:

#+BEGIN_SRC
aws s3 mb s3://YOURBUCKET-eval-rna-seqc/
aws s3 cp s3://bcbio-eval-rna-seqc/eval-rna-seqc.yaml s3://YOURBUCKET-eval-rnaseqc/
#+END_SRC
#+BEGIN_HTML
<br/>
#+END_HTML

Now ssh to the cluster head node, create the work directory and use bcbio_vm to
create a batch script that we submit to SLURM. This example uses an NFS filesystem:

#+BEGIN_SRC
bcbio_vm.py elasticluster ssh bcbio
mkdir -p ~/run/eval-rna-seqc
cd !$ && mkdir work && cd work
bcbio_vm.py ipythonprep s3://YOURBUCKET-eval-rna-seqc/eval-rna-seqc.yaml \
                        slurm cloud -r 'mincores=30' -r 'timelimit=2-00:00:00' -n 60
sbatch bcbio_submit.sh
#+END_SRC
#+BEGIN_HTML
<br/>
#+END_HTML

This will process three replicates from two different SEQC panels, performing
adapter trimming, alignment with [[star][STAR]] and produce counts, [[cufflinks][Cufflinks quantitation]]
and quality control metrics. The results will get upload back into your initial S3 bucket as
YOURBUCKET-eval-rna-seqc/final, and you can shut down the cluster used for processing.

#+LINK: seqc_paper http://www.nature.com/nbt/journal/v32/n9/full/nbt.2957.html
#+LINK: seqc_details https://bcbio-nextgen.readthedocs.org/en/latest/contents/testing.html#rnaseq-example
#+LINK: star https://github.com/alexdobin/STAR
#+LINK: cufflinks http://cufflinks.cbcb.umd.edu/


* Architecture

The implementation provides both a practical way to run large scale variant
calling and RNA-seq analysis, as well as a flexible backend architecture
suitable for production quality runs. This
writeup might feel a bit like a [[blacktriangle][black triangle moment]] since I also wrote about
[[bcbiocloudman][running bcbio on AWS three years ago]]. That implementation was a demonstration
for small scale usage rather than a production ready system. We now have a setup we can
support and run on [[bcbioscaling][large scale projects]] thanks to numerous changes in
the backend architecture:

- Amazon, and cloud based providers in general, now provide high end filesystems
  and networking. Our AWS runs are fast because they use SSD backend storage, fast
  networking connectivity and high end processors that would be difficult to
  invest in for a local cluster. Renting these is economically feasible now
  that we have an approach to provision resources, run the analysis, and tear
  everything down. The dichotomy between local cluster hardware and cloud
  hardware will continue to expand with upcoming improvements in
  [[awsc4][compute (Haswell processors)]] and [[aws16tb][storage (16Tb EBS SSD volumes]]).

- Isolating all of the software and code inside [[docker][Docker]] containers enables rapid
  pushes of fixes and improvements. From an open source support perspective,
  Amazon provides a consistent cluster environment we have full control
  over, limiting the space of potential system specific issues. From a
  researcher's perspective, this will allow use of bcbio without needing to
  spend time installing and testing locally.

- The setup runs from standard Ubuntu virtual machines using [[ansible][Ansible scripts]]
  and [[elasticluster][Elasticluster]]. This means we no longer need to support building and
  updating AMIs for changes in the architecture or code. This simplifies testing
  and pushing fixes, which should let us spend less time on support and more on
  development. It also provides a path to support bcbio on container specific
  management services like [[awsecs][Amazon's EC2 container service]].

- All long term data storage happens in [[awss3][Amazon's S3 object store]], including both
  analysis specific data as well as general reference genome data. Downloading
  reference data for an analysis on demand removes the requirement to maintain
  large shared EBS volumes. On the analysis side, you maintain only the input
  files and high value output files in S3, removing the intermediates upon
  completion of the analysis.

All of these architectural changes provide a setup that is easier to maintain
and scale over time. Our goal moving ahead is to provide a researcher
friendly interface to setting up and running analyses using [[http://galaxyproject.org/][Galaxy]]. We hope to
achieve that through the in-development [[cwl][Common Workflow Language]] from Galaxy, [[arvados][Arvados]],
[[sevenbridge][Seven Bridges]] and the [[openbio][open bioinformatics community]].

#+LINK: blacktriangle https://web.archive.org/web/20131122230658/http://rampantgames.com/blog/2004/10/black-triangle.html
#+LINK: slurm http://slurm.schedmd.com
#+LINK: bcbiocloudman https://bcbio.wordpress.com/2011/11/29/making-next-generation-sequencing-analysis-pipelines-easier-with-biocloudcentral-and-galaxy-integration/
#+LINK: bcbioscaling https://bcbio.wordpress.com/2013/05/22/scaling-variant-detection-pipelines-for-whole-genome-sequencing-analysis/
#+LINK: awsc4 http://aws.amazon.com/blogs/aws/new-c4-instances/
#+LINK: aws16tb http://www.infoq.com/news/2014/11/new-features-ec2-ebs-s3
#+LINK: awsecs http://aws.amazon.com/ecs/
#+LINK: awss3 http://aws.amazon.com/s3/
#+LINK: cwl https://github.com/rabix/common-workflow-language
#+LINK: arvados https://arvados.org/
#+LINK: sevenbridges https://www.sbgenomics.com/
#+LINK: openbio http://www.open-bio.org/wiki/Main_Page

* Timing

We ran on AWS using 64 cores with two r3.8xlarge instancs. To compare to local
compute, we also ran the same pipeline on Harvard FAS architecture using our
Lustre Regal setup. These are the timing results for running on the different
setups, split by activity. The Docker run in the top table is slightly different
than the other runs as it includes an alignment preparation step where the input
data streams in from S3 and is simultaneously bgzipped and indexed. The Docker
step also skips alignment post-processing since this relies on the non-free GATK
and we don't yet have a clean way to distribute this with bcbio and Docker. For
the remaining process steps, the Elasticluster/AWS/Docker run has similar
numbers to what we found with the setup at AstraZeneca.

** Lustre AWS + Docker + Elasticluster

| Total                     | 4:12:00 |    |
|---------------------------+---------+----|
| alignment preparation     | 0:14:00 | ** |
| alignment                 | 0:32:00 |    |
| callable regions          | 0:18:00 |    |
| alignment post-processing | 0:00:00 | ** |
| variant calling           | 2:52:00 |    |
| variant post-processing   | 0:03:00 |    |
| prepped BAM merging       | 0:00:00 | ** |
| validation                | 0:06:00 |    |
| ensemble calling          | 0:03:00 |    |

**  Lustre AWS -- AstraZeneca

| Total                     | 4:08:00 |
|---------------------------+---------|
| alignment                 | 0:28:00 |
| callable regions          | 0:45:00 |
| alignment post-processing | 0:13:00 |
| variant calling           | 2:14:00 |
| variant post-processing   | 0:03:00 |
| prepped BAM merging       | 0:05:00 |
| validation                | 0:06:00 |
| ensemble calling          | 0:03:00 |
| quality control           | 0:05:00 |

** NFS AWS -- AstraZeneca

| Total                     | 4:31:00 |
|---------------------------+---------|
| alignment                 | 0:47:00 |
| callable regions          | 0:43:00 |
| alignment post-processing | 0:13:00 |
| variant calling           | 2:16:00 |
| variant post-processing   | 0:03:00 |
| prepped BAM merging       | 0:13:00 |
| validation                | 0:06:00 |
| ensemble calling          | 0:02:00 |
| quality control           | 0:05:00 |

** Gluster AWS -- AstraZeneca

| Total                     | 5:36:00 |
|---------------------------+---------|
| alignment                 | 0:47:00 |
| callable regions          | 1:34:00 |
| alignment post-processing | 0:20:00 |
| variant calling           | 2:20:00 |
| variant post-processing   | 0:03:00 |
| prepped BAM merging       | 0:14:00 |
| validation                | 0:07:00 |
| ensemble calling          | 0:02:00 |
| quality control           | 0:05:00 |

** Lustre local -- Harvard FAS (Regal)

| Total                     | 10:30:00 |
|---------------------------+----------|
| alignment                 |  0:53:00 |
| callable regions          |  1:25:00 |
| alignment post-processing |  4:36:00 |
| variant calling           |  2:36:00 |
| variant post-processing   |  0:22:00 |
| prepped BAM merging       |  0:06:00 |
| validation                |  0:09:00 |
| ensemble calling          |  0:02:00 |
| quality control           |  0:09:00 |

* Resource usage

** CPU

Comparison of CPU usage during processing steps for Lustre, Gluster and
NFS. During alignment -- the first major processing step in all three graphs --
Gluster and NFS have a large portion of system CPU used -- the light green
lines. This usage corresponds to work by the glusterfs daemon on that machine,
likely causing the slowdowns.

#+BEGIN_HTML
<a href="http://i.imgur.com/P3sGQZg.png">
  <img src="http://i.imgur.com/P3sGQZg.png" width="650"
       alt="CPU resource usage for Lustre, Gluster and NFS">
</a>
#+END_HTML

** Network

Comparison of Network usage during processing for Lustre, Gluster and
NFS. During alignment Gluster and NFS have increased network activity,
especially input (the red line). The Gluster traffic is heavier than NFS which
is heavier than Lustre. Gluster also has a steady higher amount of network
traffic during variant calling that both NFS and Lustre do not. While this is
not saturating and does not appear to influence variant calling timing now, it
may indicate a potential bottleneck when scaling up to more simultaneous samples.

#+BEGIN_HTML
<a href="http://i.imgur.com/nVCQHcH.png">
  <img src="http://i.imgur.com/nVCQHcH.png" width="650"
       alt="Network resource usage for Lustre, Gluster and NFS">
</a>
#+END_HTML

* Costs per hour

These are the instance costs, per hour, for running a 2 node 64 core cluster and
associated Lustre filesystem. Other costs will include EBS volumes, but
these are small ($0.10/Gb/month) compared to the instance costs over these time
periods. We plan to use S3 and Glacier for long term storage rather than the
Lustre filesystem.

|                         | AWS type   | n | each  | total |
|-------------------------+------------+---+-------+-------|
| compute entry node      | m3.large   | 1 | $0.14 |       |
| compute worker nodes    | r3.8xlarge | 2 | $2.80 |       |
|                         |            |   |       | $5.73 |
| ost (object data store) | c3.2xlarge | 4 | $0.42 |       |
| mdt (metadata target)   | c3.4xlarge | 1 | $0.84 |       |
| mgt (management target) | c3.xlarge  | 1 | $0.21 |       |
| NATDevice               | m3.medium  | 1 | $0.07 |       |
| Lustre licensing        |            | 1 | $0.08 |       |
|                         |            |   |       | $2.89 |
|-------------------------+------------+---+-------+-------|
|                         |            |   |       | $8.62 |

* Work to do

To finish the automated AWS, bcbio, Docker and Lustre setup we still need to:

- Support encryption of EBS volumes for both NFS and Lustre. We encrypt data
  stored in S3.
- Support spot instances using clusterk in place of Elasticluster.
